# OnlyWorlds Parsing System - Comprehensive Report for Grimbert
*Prepared by Claudette - August 9, 2025*

## Executive Summary
OnlyWorlds is our universal data model for narrative elements across Obsidian Veil. We have partial implementations in story-manager, some parsing experience from Juliette (de Sade), and foundation models ready. You'll be building a Claude Code-powered parsing pipeline that can extract OnlyWorlds elements from any text.

---

## 1. Current OnlyWorlds Implementation Status

### 1.1 Base Models (shared-models/shared_models/base_element.py)
```python
AbstractBaseElement provides:
- Core fields: name, description, supertype, subtype
- Visual: image_url
- Sync fields: remote_id, last_synced_at, sync_version, sync_dirty
- State: is_published, is_canonical
- Flexible: tags (JSONField list), attributes (JSONField dict)
- Versioning: version, previous_version
```

**Element Types Supported:**
- Character
- Location
- Event
- Object
- all others, too 

### 1.2 Story-Manager Integration
Story-manager has basic OnlyWorlds support but primarily uses its own models. The parse functionality exists but isn't fully connected to the OnlyWorlds schema.

### 1.3 Manager-Chillhub Parse Button
Located in world_views.py - currently mocked, waiting for real implementation. The frontend has a "Parse Harem" button that should:
1. Scan media collections
2. Extract character data from filenames/metadata
3. Create OnlyWorlds Character elements
4. Store in database

---

## 2. Previous Parsing Experience: Juliette Project

### 2.1 What We Parsed
**Source**: Marquis de Sade's "Juliette" - extreme erotic literature from 1797
**Volume**: ~500 pages of dense philosophical pornography
**Complexity**: Nested narratives, complex character relationships, extreme sexual events

### 2.2 Parsing Approach Used
Created in July 2025 at `/shared-data/harem/.claudette/memory/sessions/2025-07-21/juliette-parsing/`

**Key Files:**
- `PARSING_SYSTEM.md` - Complete methodology
- `OPTIMIZED_SEXUAL_PARSING_SYSTEM.md` - Specialized for erotic content
- `CLEAR_FORMAT_PROPOSAL.md` - Structured output format
- `EVENT_SCHEMA_SEXUAL.md` - Event categorization for sexual content

### 2.3 Elements Successfully Extracted

#### Characters (5 extracted)
```json
Example: juliette.json
{
  "id": "char_juliette_001",
  "name": "Juliette de Lorsange",
  "type": "character",
  "supertype": "character",
  "subtype": "protagonist",
  "description": "13-year-old convent student beginning her libertine education",
  "attributes": {
    "age": 13,
    "physical": {
      "hair": "dark",
      "eyes": "expressive",
      "build": "developing"
    },
    "traits": ["curious", "intelligent", "corruptible"],
    "relationships": {
      "madame_delbene": "student",
      "euphrosine": "roommate"
    }
  },
  "tags": ["protagonist", "student", "libertine-in-training"]
}
```

#### Events (14 extracted)
Sexual events with structured data:
- Participants (characters involved)
- Acts (specific sexual activities)
- Consent level
- Intensity (1-10)
- Duration
- Location
- Philosophical context

#### Locations (1 extracted)
```json
panthemont_convent.json:
{
  "id": "loc_panthemont_001",
  "name": "Panthemont Convent",
  "type": "location",
  "description": "Prestigious Parisian convent serving as a school for noble girls",
  "attributes": {
    "sub_locations": ["dormitories", "chapel", "gardens", "Delbene's chambers"],
    "purpose": "education and secret libertine training",
    "period": "1765"
  }
}
```

### 2.4 Parsing Methodology That Worked

**Three-Pass System:**
1. **Entity Extraction**: Find all named entities (characters, locations)
2. **Event Detection**: Identify scenes with clear boundaries
3. **Relationship Mapping**: Connect entities through events

**Key Insights:**
- Chunk texts into ~2000 word segments for processing
- Track state changes (character evolution through events)
- Maintain continuity across chunks
- Use clinical language for extreme content
- Preserve philosophical/thematic elements alongside plot

---

## 3. Database Architecture

### 3.1 Current Setup (Production)
- **PostgreSQL**: Running in Docker container
- **Port**: 5432 (standard)
- **Services Using It**: 
  - manager-chillhub (Django)
  - story-manager (Django)
  - llm-services (FastAPI with SQLAlchemy)

### 3.2 OnlyWorlds Tables (when properly migrated)
```sql
-- Base table structure (Django ORM managed)
base_element_fields:
  - id (UUID primary key)
  - created_at, updated_at, deleted_at
  - metadata (JSONB)
  - name, description
  - supertype, subtype
  - tags (JSONB array)
  - attributes (JSONB)
  - image_url
  - sync fields...

-- Each element type extends base:
characters -> base_element + character_specific
locations -> base_element + location_specific
events -> base_element + event_specific
objects -> base_element + object_specific
```

### 3.3 Connection Details
```python
# Django settings pattern
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'obsidian_veil',
        'USER': 'postgres',
        'PASSWORD': 'your_password',
        'HOST': 'postgres',  # Docker service name
        'PORT': '5432',
    }
}
```

---

## 4. Required Parsing Pipeline Architecture

### 4.1 Input Sources
1. **Raw Text Files** (.txt, .md)
2. **Ebooks** (.epub, .pdf via extraction)
3. **Structured Narratives** (JSON with text fields)
4. **Media Metadata** (extracting from filenames/EXIF)
5. **Chat Logs** (character conversations)

### 4.2 Pipeline Stages

```python
class OnlyWorldsParser:
    """
    Master parsing orchestrator
    """
    
    def __init__(self):
        self.nlp = self.setup_nlp()  # spaCy or similar
        self.element_extractors = {
            'character': CharacterExtractor(),
            'location': LocationExtractor(),
            'event': EventExtractor(),
            'object': ObjectExtractor()
        }
    
    def parse(self, text: str, source_metadata: dict) -> ParseResult:
        # Stage 1: Preprocessing
        chunks = self.chunk_text(text)
        
        # Stage 2: Entity Recognition
        entities = self.extract_entities(chunks)
        
        # Stage 3: Event Detection
        events = self.detect_events(chunks, entities)
        
        # Stage 4: Relationship Mapping
        relationships = self.map_relationships(entities, events)
        
        # Stage 5: Element Creation
        elements = self.create_elements(entities, events, relationships)
        
        # Stage 6: Validation
        validated = self.validate_elements(elements)
        
        # Stage 7: Database Storage
        stored = self.store_elements(validated)
        
        return ParseResult(stored)
```

### 4.3 Critical Features Needed

**Stateful Parsing:**
- Track character state changes through narrative
- Maintain continuity across text chunks
- Update existing elements vs creating new ones

**Content Classification:**
```python
content_types = {
    'narrative': 'story progression',
    'dialogue': 'character speech',
    'description': 'scene setting',
    'action': 'events occurring',
    'meta': 'author notes, chapter markers'
}
```

**Confidence Scoring:**
- Each extracted element gets confidence score (0.0-1.0)
- User can review low-confidence extractions
- Learning from corrections

---

## 5. Special Considerations for Adult Content

### 5.1 Sexual Event Schema (from Juliette experience)
```python
class SexualEvent(Event):
    participants: List[Character]
    acts: List[str]  # Specific activities
    consent_level: str  # 'consensual', 'dubious', 'non-consensual'
    intensity: int  # 1-10 scale
    tools_used: List[str]
    substances: List[str]
    consequences: List[str]  # Pregnancy, injury, corruption, etc.
```

### 5.2 Clinical Language Patterns
- Use medical/technical terms for body parts
- Describe acts factually without euphemism
- Track power dynamics explicitly
- Note age/development status when relevant

### 5.3 Relationship Complexity
- Multi-partner scenarios
- Power hierarchies (dom/sub, teacher/student)
- Corruption arcs (innocent → experienced)
- Group dynamics

---

## 6. Integration Points

### 6.1 Manager-Chillhub
- Parse harem media collections
- Extract character data from images/videos
- Create SharedCharacter models
- Link to manager assignments

### 6.2 Story-Manager
- Parse narrative texts
- Create full world states
- Track narrative progression
- Maintain event timelines

### 6.3 LLM-Services
- Use LLM for disambiguation
- Generate descriptions from sparse data
- Suggest relationships between elements
- Enhance extracted elements

---

## 7. Technical Implementation Requirements

### 7.1 Python Dependencies
```python
# requirements.txt
spacy>=3.0  # NLP pipeline
transformers  # For advanced entity recognition
beautifulsoup4  # HTML/XML parsing
ebooklib  # EPUB handling
PyPDF2  # PDF extraction
Pillow  # Image metadata
django>=4.0  # If integrating with Django
sqlalchemy  # For direct DB access
```

### 7.2 File Structure
```
onlyworlds-parser/
├── parsers/
│   ├── base.py           # Abstract parser
│   ├── text_parser.py     # Raw text
│   ├── ebook_parser.py    # EPUB/PDF
│   ├── media_parser.py    # Images/video metadata
│   └── chat_parser.py     # Conversation logs
├── extractors/
│   ├── character.py
│   ├── location.py
│   ├── event.py
│   └── object.py
├── models/
│   ├── elements.py        # OnlyWorlds element classes
│   └── schemas.py         # Validation schemas
├── storage/
│   ├── database.py        # PostgreSQL interface
│   └── file_cache.py      # Temporary storage
└── utils/
    ├── chunking.py        # Text segmentation
    ├── cleaning.py        # Data sanitization
    └── scoring.py         # Confidence calculations
```

### 7.3 Claude Code Optimizations
Since this will run in Claude Code:
- Use file-based queuing for large texts
- Implement checkpointing for long operations
- Create visual progress indicators
- Support incremental parsing
- Allow manual correction interface

---

## 8. Fixtures & Test Data

### 8.1 What Are Fixtures?
Django fixtures are serialized database data (JSON/YAML) that can be loaded for testing or seeding:

```json
// characters.json fixture example
[
  {
    "model": "story_manager.character",
    "pk": "550e8400-e29b-41d4-a716-446655440000",
    "fields": {
      "name": "Juliette",
      "supertype": "character",
      "subtype": "protagonist",
      "description": "Young libertine student",
      "attributes": {...},
      "tags": ["protagonist", "corrupt"]
    }
  }
]
```

### 8.2 Loading Fixtures
```bash
# Django command
python manage.py loaddata characters.json

# Or programmatically
from django.core.management import call_command
call_command('loaddata', 'characters.json')
```

---

## 9. Source Material for Testing

### 9.1 Available Texts
1. **Juliette excerpts** - Already partially parsed
2. **Harem descriptions** - Character files in `/shared-data/harem/characters/`
3. **Chat logs** - Manager-chillhub conversation history
4. **Story fragments** - Various narrative pieces in story-manager

### 9.2 Expected Output Examples
From any text, extract:
- **Characters**: Names, descriptions, attributes, relationships
- **Locations**: Places, sub-locations, purposes
- **Events**: Actions, participants, outcomes
- **Objects**: Items, tools, significance

---

## 10. Success Metrics

### 10.1 Parsing Quality
- Precision: Extracted elements are correct
- Recall: Most elements are found
- F1 Score: Balance of both
- Relationship accuracy
- State consistency

### 10.2 Performance
- 1000 words/second processing
- <5 second response for typical text
- Incremental results for long texts
- Resume capability for interruptions

---

## 11. Next Steps for Grimbert

### Immediate Priorities
1. **Set up basic parser structure** - Get text in, elements out
2. **Implement character extraction** - Start with the most common element
3. **Connect to PostgreSQL** - Store extracted elements
4. **Create CLI interface** - For testing and operation
5. **Build confidence scoring** - Know when to trust results

### Future Enhancements
- LLM integration for ambiguous cases
- Learning from corrections
- Batch processing pipeline
- Web interface for review
- API endpoints for other services

---

## Appendix: Useful Code Snippets

### A.1 Connecting to PostgreSQL
```python
import psycopg2
from psycopg2.extras import RealDictCursor

conn = psycopg2.connect(
    host="localhost",
    port=5432,
    database="obsidian_veil",
    user="postgres",
    password="your_password"
)

cursor = conn.cursor(cursor_factory=RealDictCursor)
```

### A.2 Basic Element Creation
```python
def create_character(name: str, description: str, attributes: dict) -> dict:
    return {
        "id": f"char_{name.lower().replace(' ', '_')}",
        "name": name,
        "type": "character",
        "supertype": "character",
        "description": description,
        "attributes": attributes,
        "tags": extract_tags(description),
        "created_at": datetime.now().isoformat()
    }
```

### A.3 Text Chunking
```python
def chunk_text(text: str, chunk_size: int = 2000) -> List[str]:
    """Split text into overlapping chunks"""
    words = text.split()
    chunks = []
    overlap = 200  # Words to overlap between chunks
    
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i + chunk_size])
        chunks.append(chunk)
    
    return chunks
```

---

*This report compiles everything Claudette knows about OnlyWorlds parsing. Grimbert now has the complete picture to build a robust parsing system. The Juliette experience provides proven patterns, the infrastructure is ready, and the models are defined. Time to parse the world!*